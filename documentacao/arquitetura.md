# ğŸ—ï¸ Documento de Arquitetura - Pipeline de Big Data

## Projeto: AnÃ¡lise de ProduÃ§Ã£o CinematogrÃ¡fica

**Data**: Outubro 2025  
**VersÃ£o**: 1.0  
**Status**: Em Desenvolvimento (AV1)

---

## 1. VisÃ£o Geral

Este documento descreve a arquitetura do pipeline de Big Data implementado para anÃ¡lise da indÃºstria cinematogrÃ¡fica. O projeto segue as melhores prÃ¡ticas de engenharia de dados, incluindo a arquitetura medalhÃ£o (Bronze/Silver/Gold) e processamento em lotes (batch).

## 2. Arquitetura do Pipeline

### 2.1 Diagrama do Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PIPELINE DE BIG DATA                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. DATA SOURCES â”‚
â”‚   (Fontes)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ movies_        â”‚
â”‚   metadata.csv   â”‚
â”‚ â€¢ credits.csv    â”‚
â”‚                  â”‚
â”‚ Volume: ~45k     â”‚
â”‚ Tipo: CSV        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. INGESTION    â”‚
â”‚   (IngestÃ£o)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Pandas         â”‚
â”‚ â€¢ read_csv()     â”‚
â”‚ â€¢ Batch mode     â”‚
â”‚                  â”‚
â”‚ Output: DataFramesâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. TRANSFORM     â”‚
â”‚  (TransformaÃ§Ã£o) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Limpeza:         â”‚
â”‚ â€¢ Drop nulls     â”‚
â”‚ â€¢ Type casting   â”‚
â”‚ â€¢ Validation     â”‚
â”‚                  â”‚
â”‚ Enriquecimento:  â”‚
â”‚ â€¢ ROI calc       â”‚
â”‚ â€¢ Date parsing   â”‚
â”‚ â€¢ JSON extract   â”‚
â”‚                  â”‚
â”‚ IntegraÃ§Ã£o:      â”‚
â”‚ â€¢ Merge datasets â”‚
â”‚ â€¢ Join by ID     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. LOADING      â”‚
â”‚  (Carregamento)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Arquitetura      â”‚
â”‚ MedalhÃ£o:        â”‚
â”‚                  â”‚
â”‚ /bronze/         â”‚
â”‚ â””â”€ Raw data      â”‚
â”‚                  â”‚
â”‚ /silver/         â”‚
â”‚ â””â”€ Clean data    â”‚
â”‚                  â”‚
â”‚ /gold/           â”‚
â”‚ â””â”€ Analytics     â”‚
â”‚                  â”‚
â”‚ Formatos:        â”‚
â”‚ â€¢ CSV            â”‚
â”‚ â€¢ Parquet        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. DESTINATION   â”‚
â”‚    (Destino)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Jupyter        â”‚
â”‚   Notebook       â”‚
â”‚ â€¢ Matplotlib     â”‚
â”‚ â€¢ Seaborn        â”‚
â”‚                  â”‚
â”‚ Outputs:         â”‚
â”‚ â€¢ Dashboards     â”‚
â”‚ â€¢ Insights       â”‚
â”‚ â€¢ Reports        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Camadas Detalhadas

#### **Camada 1: Fontes de Dados**
- **Tipo**: Arquivos CSV locais
- **Volume**: ~45.000 registros
- **AtualizaÃ§Ã£o**: EstÃ¡tica (batch)
- **Formato**: Estruturado e semi-estruturado (JSON em campos)

#### **Camada 2: IngestÃ£o**
- **Tecnologia**: Pandas
- **MÃ©todo**: Batch processing
- **FrequÃªncia**: Sob demanda
- **ValidaÃ§Ã£o**: Schema inference automÃ¡tico

#### **Camada 3: TransformaÃ§Ã£o**
- **OperaÃ§Ãµes**:
  1. Limpeza de dados
  2. NormalizaÃ§Ã£o de tipos
  3. CriaÃ§Ã£o de features
  4. Processamento de JSON
  5. IntegraÃ§Ã£o de datasets

#### **Camada 4: Carregamento**
- **PadrÃ£o**: Arquitetura MedalhÃ£o
- **Formatos**: CSV (compatibilidade) + Parquet (performance)
- **OrganizaÃ§Ã£o**: HierÃ¡rquica por qualidade de dados

#### **Camada 5: Destino**
- **Interface**: Jupyter Notebook
- **VisualizaÃ§Ãµes**: GrÃ¡ficos interativos
- **AnÃ¡lises**: EstatÃ­sticas descritivas e insights

---

## 3. Tecnologias Utilizadas

### 3.1 Stack Atual (Implementado)

| Camada | Tecnologia | Justificativa | Status |
|--------|------------|---------------|--------|
| **IngestÃ£o** | Pandas | Facilidade de uso, boa para batch | âœ… Implementado |
| **TransformaÃ§Ã£o** | Pandas + NumPy | Processamento eficiente, vetorizaÃ§Ã£o | âœ… Implementado |
| **Armazenamento** | CSV + Parquet | Compatibilidade + Performance | âœ… Implementado |
| **VisualizaÃ§Ã£o** | Matplotlib + Seaborn | GrÃ¡ficos de alta qualidade | âœ… Implementado |
| **OrquestraÃ§Ã£o** | Manual (Jupyter) | PrototipaÃ§Ã£o rÃ¡pida | âœ… Implementado |

### 3.2 Tecnologias Alternativas (Refinamento Futuro)

| Camada | Tecnologia Paga/EscalÃ¡vel | BenefÃ­cio |
|--------|---------------------------|-----------|
| **IngestÃ£o** | Apache Kafka | Streaming em tempo real |
| **Processamento** | Apache Spark | Processamento distribuÃ­do |
| **Armazenamento** | AWS S3 / Azure Data Lake | Escalabilidade, durabilidade |
| **Data Warehouse** | Google BigQuery / Snowflake | Consultas SQL em escala |
| **OrquestraÃ§Ã£o** | Apache Airflow | Agendamento, monitoramento |
| **VisualizaÃ§Ã£o** | Tableau / Power BI | Dashboards profissionais |

### 3.3 Justificativa das Escolhas

#### **Por que Pandas?**
- âœ… Curva de aprendizado baixa
- âœ… IntegraÃ§Ã£o nativa com NumPy e Matplotlib
- âœ… Suficiente para datasets de atÃ© ~1M linhas
- âš ï¸ LimitaÃ§Ã£o: NÃ£o distribuÃ­do (single-machine)

#### **Por que Arquitetura MedalhÃ£o?**
- âœ… SeparaÃ§Ã£o clara de responsabilidades
- âœ… Reprodutibilidade (dados brutos preservados)
- âœ… Qualidade progressiva (Bronze â†’ Silver â†’ Gold)
- âœ… PadrÃ£o da indÃºstria (Databricks, Delta Lake)

#### **Por que CSV + Parquet?**
- âœ… CSV: LegÃ­vel, amplamente compatÃ­vel
- âœ… Parquet: CompressÃ£o eficiente, queries rÃ¡pidas
- âœ… Dual-format: Flexibilidade de uso

---

## 4. DivisÃ£o de Tarefas da Equipe

| Membro | Responsabilidade | Tarefas |
|--------|------------------|---------|
| **[Membro 1]** | IngestÃ£o + TransformaÃ§Ã£o | â€¢ Carregamento de dados<br>â€¢ Limpeza e validaÃ§Ã£o<br>â€¢ Processamento de JSON |
| **[Membro 2]** | TransformaÃ§Ã£o + Carregamento | â€¢ CriaÃ§Ã£o de features<br>â€¢ IntegraÃ§Ã£o de datasets<br>â€¢ Salvamento em medalhÃ£o |
| **[Membro 3]** | Destino + DocumentaÃ§Ã£o | â€¢ VisualizaÃ§Ãµes<br>â€¢ AnÃ¡lises estatÃ­sticas<br>â€¢ README e arquitetura |

**ColaboraÃ§Ã£o**: Todos os membros contribuÃ­ram com cÃ³digo, revisÃµes e documentaÃ§Ã£o atravÃ©s do Git.

---

## 5. Arquitetura Parcial Implementada (AV1)

### âœ… Implementado

- [x] IngestÃ£o de dados CSV
- [x] Limpeza e normalizaÃ§Ã£o
- [x] Enriquecimento (ROI, categorias, datas)
- [x] Processamento de JSON (gÃªneros, elenco)
- [x] IntegraÃ§Ã£o de datasets
- [x] Arquitetura medalhÃ£o (Bronze/Silver/Gold)
- [x] Salvamento em CSV e Parquet
- [x] VisualizaÃ§Ãµes analÃ­ticas
- [x] DocumentaÃ§Ã£o completa

### â³ Planejado (PrÃ³ximas IteraÃ§Ãµes)

- [ ] SimulaÃ§Ã£o de streaming
- [ ] IntegraÃ§Ã£o com APIs externas (TMDb API)
- [ ] Machine Learning (prediÃ§Ã£o de sucesso)
- [ ] Dashboard interativo (Plotly/Dash)
- [ ] Deploy em cloud

---

## 6. Fluxo de Dados Detalhado

### 6.1 IngestÃ£o

```python
# Input: Arquivos CSV
movies_raw = pd.read_csv('movies_metadata.csv')
credits_raw = pd.read_csv('credits.csv')

# Output: DataFrames in-memory
```

**CaracterÃ­sticas**:
- Modo: Batch
- FrequÃªncia: Sob demanda
- Volume: ~45k linhas

### 6.2 TransformaÃ§Ã£o

```python
# Limpeza
movies.dropna(subset=['id', 'title'])
movies['budget'] = pd.to_numeric(movies['budget'], errors='coerce')

# Enriquecimento
movies['roi'] = (revenue - budget) / budget * 100
movies['ano_lancamento'] = pd.to_datetime(release_date).dt.year

# IntegraÃ§Ã£o
dados_integrados = movies.merge(credits, on='id')
```

**OperaÃ§Ãµes**:
1. Drop de valores nulos crÃ­ticos
2. ConversÃ£o de tipos
3. CriaÃ§Ã£o de colunas derivadas
4. ExtraÃ§Ã£o de JSON
5. Merge de datasets

### 6.3 Carregamento

```python
# Bronze (Raw)
movies_raw.to_csv('dados/bronze/movies_raw.csv')

# Silver (Clean)
movies.to_csv('dados/silver/movies_cleaned.csv')

# Gold (Analytics)
dados_integrados.to_parquet('dados/gold/filmes_analise.parquet')
```

**OrganizaÃ§Ã£o**:
- Bronze: PreservaÃ§Ã£o de dados originais
- Silver: Dados confiÃ¡veis e limpos
- Gold: Dados otimizados para anÃ¡lise

---

## 7. Qualidade de Dados

### 7.1 ValidaÃ§Ãµes Implementadas

| ValidaÃ§Ã£o | MÃ©todo | Status |
|-----------|--------|--------|
| Valores nulos | `dropna()` | âœ… |
| Tipos incorretos | `pd.to_numeric()` | âœ… |
| Duplicatas | VerificaÃ§Ã£o de IDs Ãºnicos | âœ… |
| Outliers | AnÃ¡lise visual | â³ |

### 7.2 MÃ©tricas de Qualidade

```
Dataset Original: 45,476 filmes
ApÃ³s Limpeza:     ~42,000 filmes (92% retidos)
Completude:       >95% em campos crÃ­ticos
```

---

## 8. Performance e Escalabilidade

### 8.1 MÃ©tricas Atuais

- **Tempo de IngestÃ£o**: ~5 segundos
- **Tempo de TransformaÃ§Ã£o**: ~15 segundos
- **Tempo de Salvamento**: ~10 segundos
- **Tempo Total**: ~30 segundos

### 8.2 LimitaÃ§Ãµes Atuais

- Processamento single-thread (Pandas)
- Limite de memÃ³ria RAM (~16GB recomendado)
- NÃ£o suporta streaming

### 8.3 Plano de Escalabilidade

**Para volumes > 1M linhas**:
1. Migrar para PySpark (processamento distribuÃ­do)
2. Usar Parquet como formato primÃ¡rio
3. Implementar particionamento de dados
4. Considerar cloud computing (AWS EMR, Databricks)

---

## 9. SeguranÃ§a e GovernanÃ§a

### 9.1 Dados PÃºblicos

- Todos os dados sÃ£o pÃºblicos (TMDb)
- Sem informaÃ§Ãµes sensÃ­veis (LGPD/GDPR compliant)

### 9.2 Versionamento

- CÃ³digo: Git/GitHub
- Dados: Arquitetura medalhÃ£o preserva histÃ³rico

---

## 10. ConclusÃ£o

A arquitetura implementada demonstra um pipeline de Big Data funcional e bem estruturado, adequado para o escopo acadÃªmico e extensÃ­vel para aplicaÃ§Ãµes reais. A escolha de tecnologias prioriza facilidade de aprendizado e implementaÃ§Ã£o, enquanto mantÃ©m portas abertas para escalabilidade futura.

**PrÃ³ximos Marcos**:
- AV1: âœ… Pipeline completo implementado
- AV2: Adicionar ML e deploy em cloud
- Projeto Final: Dashboard produÃ§Ã£o e apresentaÃ§Ã£o

---

**Autores**: [Equipe do Projeto]  
**Ãšltima RevisÃ£o**: Outubro 2025

